{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pickle\n",
    "from collections.abc import Iterator\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pandas import DataFrame\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(\n",
    "    user,\n",
    "    password,\n",
    "    account,\n",
    "    warehouse,\n",
    "    database,\n",
    "    schema,\n",
    "    query,\n",
    "    how=\"all\",\n",
    "    batch_size=10000,\n",
    "    conn = None\n",
    ") -> DataFrame | Iterator[DataFrame]:\n",
    "    \"\"\"\n",
    "    This function will return a pandas dataframe from a query\n",
    "    \"\"\"\n",
    "    if conn == None:\n",
    "    # Create a connection object\n",
    "        conn = snowflake.connector.connect(\n",
    "            user=user,\n",
    "            password=password,\n",
    "            account=account,\n",
    "            warehouse=warehouse,\n",
    "            database=database,\n",
    "            schema=schema,\n",
    "        )\n",
    "    # Create a cursor object\n",
    "    cur = conn.cursor()\n",
    "    # Execute the query\n",
    "    cur.execute(query)\n",
    "\n",
    "    if how == \"all\":\n",
    "        # Fetch the results\n",
    "        # Fetch the result set from the cursor and deliver it as the Pandas DataFrame.\n",
    "        df = cur.fetch_pandas_all()\n",
    "    elif how == \"many\":\n",
    "        # Fetch the results\n",
    "        df = cur.fetch_pandas_batches(batch_size=batch_size)\n",
    "\n",
    "    # Close the connection\n",
    "    # conn.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = USER\n",
    "password = PASSWORD\n",
    "account = ACCOUNT\n",
    "warehouse = WAREHOUSE\n",
    "database = 'TEMPORARY_DATA'\n",
    "schema = 'BIDEM'\n",
    "uploading_conn = snowflake.connector.connect(\n",
    "    user=user,\n",
    "    password=password,\n",
    "    account=account,\n",
    "    warehouse=warehouse,\n",
    "    database=database,\n",
    "    schema=schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_query(\n",
    "    user=USER,\n",
    "    password=PASSWORD,\n",
    "    account=ACCOUNT,\n",
    "    warehouse=WAREHOUSE,\n",
    "    database=\"TEMPORARY_DATA\",\n",
    "    schema=\"BIDEM\",\n",
    "    query=\"\"\"\n",
    "WITH filtered_experience AS (\n",
    "    SELECT \n",
    "        BGI_ONET_NAME, \n",
    "        PERSON_ID, \n",
    "        ID, \n",
    "        BGI_START_DATE, \n",
    "        BGI_END_DATE,\n",
    "        BGI_NO_TITLE_FLAG\n",
    "    FROM PDL_CLEAN.V4.EXPERIENCE\n",
    "    WHERE BGI_JOB_COUNTRY = 'United States'\n",
    "      AND BGI_ONET_CONFIDENCE > 50\n",
    ")\n",
    "SELECT *\n",
    "FROM filtered_experience\n",
    "WHERE PERSON_ID IN (\n",
    "    SELECT PERSON_ID\n",
    "    FROM filtered_experience\n",
    "    GROUP BY PERSON_ID\n",
    "    HAVING COUNT(*) > 1 \n",
    "       AND COUNT(*) < 20\n",
    "       -- Exclude any person with an experience having BGI_NO_TITLE_FLAG = false\n",
    "       AND SUM(CASE WHEN BGI_NO_TITLE_FLAG = FALSE THEN 1 ELSE 0 END) = 0\n",
    "\t   -- Exclude any person with an experience having BGI_ONET_NAME = 'Unclassified'\n",
    "       AND SUM(CASE WHEN BGI_ONET_NAME = 'Unclassified' THEN 1 ELSE 0 END) = 0\n",
    "       -- Exclude any person with an experience (with both start and end dates) lasting less than a year\n",
    "       AND SUM(CASE \n",
    "                WHEN BGI_END_DATE IS NOT NULL \n",
    "                     AND DATEDIFF(day, BGI_START_DATE, BGI_END_DATE) < 365 \n",
    "                THEN 1 \n",
    "                ELSE 0 \n",
    "             END) = 0\n",
    ")\n",
    "ORDER BY PERSON_ID, BGI_START_DATE ASC\n",
    ";\n",
    "\"\"\",\n",
    "conn=uploading_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54892286, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume df is already loaded. Convert date columns to datetime\n",
    "df['BGI_START_DATE'] = pd.to_datetime(df['BGI_START_DATE'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before the next step, make BGI_ONET_NAME categorical\n",
    "df['BGI_ONET_NAME'] = df['BGI_ONET_NAME'].astype('category')\n",
    "\n",
    "# encode BGI_ONET_NAME\n",
    "df['BGI_ONET_NAME_ENCODED'] = df['BGI_ONET_NAME'].cat.codes\n",
    "\n",
    "# save the encoding\n",
    "onet_name_encoding = df[['BGI_ONET_NAME', 'BGI_ONET_NAME_ENCODED']].drop_duplicates().sort_values('BGI_ONET_NAME_ENCODED').reset_index(drop=True)\n",
    "onet_name_encoding.to_parquet('data/onet_name_encoding.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_occupation_sequences(df):\n",
    "    # Sort the entire dataframe by PERSON_ID and BGI_START_DATE once\n",
    "    df_sorted = df.sort_values(['PERSON_ID', 'BGI_START_DATE'])\n",
    "    \n",
    "    # Group by PERSON_ID and convert each group's BGI_ONET_NAME_ENCODED to a numpy array\n",
    "    sequences = (\n",
    "        df_sorted.groupby('PERSON_ID')['BGI_ONET_NAME_ENCODED']\n",
    "        .apply(lambda x: x.to_numpy(dtype=np.int16))\n",
    "        .tolist()\n",
    "    )\n",
    "    \n",
    "    return sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataframe and get sequences\n",
    "sequences = create_occupation_sequences(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sequences as a pickle file\n",
    "with open('data/occupation_sequences.pkl', 'wb') as f:\n",
    "    pickle.dump(sequences, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skills",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
